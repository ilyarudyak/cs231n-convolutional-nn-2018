{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.rnn_layers import *\n",
    "from cs231n.captioning_solver import CaptioningSolver\n",
    "from cs231n.classifiers.rnn import CaptioningRNN\n",
    "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from cs231n.image_utils import image_from_url\n",
    "from cs231n.layers import *\n",
    "from cs231n.rnn_layers import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty involved question so let's do some additional debugging. Hopefully it will also be useful for tackling captioning in `pytorch` on a real (`20G`) dataset (an upcoming project from `Udacity`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microsoft COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look at our dataset:\n",
    "\n",
    "- first of all we're working with 2014 dataset; `80K` training images (we skip here our validation statistics for simplicity); \n",
    "- we have features from `VGG16` (full and reduced) as described in the assignment; features are `pca` reduced;\n",
    "- we have `5` captions per image: `400K` captions; captions padded or truncated to `17` words; represented as list of indicies from a vocabulary; we may see their content with `decode_captions()`; we have files with somewhat misleading titles `images` - they contain the number of an image for each caption;\n",
    "- we have an url for each training example, not an image itself as specified in the assignments;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last question that we have - is there any difference between those files and original files (except dimesionality reduction)? That's not quite clear from files itself so let's postpone this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.home() / 'data/coco_captioning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/ilyarudyak/data/coco_captioning/val2014_urls.txt'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/coco2014_vocab.json'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/train2014_urls.txt'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/val2014_vgg16_fc7.h5'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/train2014_vgg16_fc7.h5'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/val2014_vgg16_fc7_pca.h5'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/train2014_images.txt'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/val2014_images.txt'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/coco2014_captions.h5'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/train2014_vgg16_fc7_pca.h5')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_dir.glob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/ilyarudyak/data/coco_captioning/val2014_vgg16_fc7.h5'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/train2014_vgg16_fc7.h5'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/val2014_vgg16_fc7_pca.h5'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/train2014_vgg16_fc7_pca.h5')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_dir.glob('*fc7*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/ilyarudyak/data/coco_captioning/coco2014_vocab.json')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_dir.glob('*vocab*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/ilyarudyak/data/coco_captioning/coco2014_captions.h5')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_dir.glob('*captions*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/ilyarudyak/data/coco_captioning/train2014_images.txt'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/val2014_images.txt')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_dir.glob('*images*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/ilyarudyak/data/coco_captioning/val2014_urls.txt'),\n",
       " PosixPath('/Users/ilyarudyak/data/coco_captioning/train2014_urls.txt')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_dir.glob('*urls*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_coco_data(pca_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_captions', 'train_image_idxs', 'val_captions', 'val_image_idxs', 'train_features', 'val_features', 'idx_to_word', 'word_to_idx', 'train_urls', 'val_urls'])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82783, 512)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_features'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['idx_to_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<NULL>', '<START>', '<END>', '<UNK>', 'a', 'on', 'of', 'the', 'in', 'with']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data['idx_to_word'][i] for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400135, 17)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_captions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   4, 142, 510,  10, 667, 415, 277,  58,   2,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_captions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> a very clean and well decorated empty bathroom <END>'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_captions(data['train_captions'][0], data['idx_to_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<NULL>'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['idx_to_word'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82783,)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_urls'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://farm4.staticflickr.com/3153/2970773875_164f0c0b83_z.jpg'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_urls'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = data['train_urls'][0]\n",
    "# plt.imshow(image_from_url(url));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400135,)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_image_idxs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53314, 21548, 53314, 21548, 43077, 43077, 53314, 53314, 44042,\n",
       "       16413], dtype=int32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_image_idxs'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = data['train_urls'][43077]\n",
    "# plt.imshow(image_from_url(url));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START> a <UNK> stop sign across the street from a red car <END>',\n",
       " '<START> a <UNK> stop sign and a red <UNK> on the road <END>']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_captions(data['train_captions'][4:6], data['idx_to_word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally extract all captions for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps = data['train_captions'][data['train_image_idxs'] == 43077]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START> a <UNK> stop sign across the street from a red car <END>',\n",
       " '<START> a <UNK> stop sign and a red <UNK> on the road <END>',\n",
       " '<START> a red stop sign with a <UNK> <UNK> <UNK> under the <UNK> stop <END>',\n",
       " '<START> a stop sign that has been <UNK> is <UNK> in front of a parked car <END>',\n",
       " '<START> a street sign <UNK> to <UNK> stop <UNK> <END>']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_captions(caps, data['idx_to_word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like we have only 50 images and 1 caption per image in this small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "small_data = load_coco_data(max_train=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_captions', 'train_image_idxs', 'val_captions', 'val_image_idxs', 'train_features', 'val_features', 'idx_to_word', 'word_to_idx', 'train_urls', 'val_urls'])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82783, 512)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data['train_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 17)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data['train_captions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data['train_image_idxs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2844, 72739, 29132, 24456, 19151,  9391, 70313, 68609, 43887,\n",
       "        1509, 46755, 12770, 47533, 22781,  2198, 21214, 77700, 19257,\n",
       "       77186, 45683, 78192, 31889, 55132, 48245,  4805, 17209, 67353,\n",
       "       32150, 78473, 29969, 30195, 77955, 60566,  4192, 60633, 52886,\n",
       "       25245, 12033, 41517, 67383, 63836, 14180, 73870, 26231, 43160,\n",
       "        1457, 62170, 75613, 45350, 49583], dtype=int32)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data['train_image_idxs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82783,)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data['train_urls'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_data['idx_to_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = small_data['train_urls'][2844]\n",
    "# plt.imshow(image_from_url(url));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> a living room with <UNK> of <UNK> <UNK> <END>'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_captions(small_data['train_captions'][0], small_data['idx_to_word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to make a step of forward pass using this small dataset. When we create a model we initialize only weights, we nave no layers like in `pytorch`. All our layers are functions here, not classes. What layers do we have?\n",
    "\n",
    "- linear layer to get initial hidden state of our `RNN` from `CNN` features; in our case this is a square matrix - `hidden_dim=512` is the same as `fc7` features (with reduced dimensionality);\n",
    "- embedding layer with shape `(vocab_size, wordvec_dim)` in our case `(1004, 256)`; \n",
    "- rnn layer with shapes for `Wh` and `Wx`: (512, 512), (256, 512); we multiply `x` as a row vector: `next_h_lin = prev_h.dot(Wh) + x.dot(Wx) + b`;\n",
    "- finally we produce our scores using `W_vocab` of shape `(hidden_dim, vocab_size)` or in our case `(512, 1004)`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "small_data = load_coco_data(max_train=50)\n",
    "small_rnn_model = CaptioningRNN(\n",
    "          cell_type='rnn',\n",
    "          word_to_idx=data['word_to_idx'],\n",
    "          input_dim=data['train_features'].shape[1],\n",
    "          hidden_dim=512,\n",
    "          wordvec_dim=256,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['W_embed', 'W_proj', 'b_proj', 'Wx', 'Wh', 'b', 'W_vocab', 'b_vocab'])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_rnn_model.params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_rnn_model.params['W_proj'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1004, 256)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_rnn_model.params['W_embed'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((512, 512), (256, 512))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_rnn_model.params['Wh'].shape, small_rnn_model.params['Wx'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 1004)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_rnn_model.params['W_vocab'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get a minibatch of data and make a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = sample_coco_minibatch(small_data,\n",
    "                                  batch_size=25,\n",
    "                                  split='train')\n",
    "captions, features, urls = minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 17), (25, 512), (25,))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.shape, features.shape, urls.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### captions and mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we have to produce captions for input (`captions_in`) and for loss (`captions_out`). It's important to notice that in case of `captions_in` we just remove the last `0` (in many cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_in = captions[:, :-1]\n",
    "captions_out = captions[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   4,  12, 292,   9,  40, 236, 628,   8,  44,   3,   9, 318,\n",
       "         2,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   4,  12, 292,   9,  40, 236, 628,   8,  44,   3,   9, 318,\n",
       "         2,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_in[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,  12, 292,   9,  40, 236, 628,   8,  44,   3,   9, 318,   2,\n",
       "         0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_out[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at `mask` that we use to compute loss. It's allowed us to exclude padding from loss computations. `mask` has the same shape as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_rnn_model._null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (captions_out != small_rnn_model._null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 16)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False, False])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack our weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight and bias for the affine transform from image features to initial\n",
    "# hidden state\n",
    "W_proj, b_proj = small_rnn_model.params['W_proj'], small_rnn_model.params['b_proj']\n",
    "\n",
    "# Word embedding matrix\n",
    "W_embed = small_rnn_model.params['W_embed']\n",
    "\n",
    "# Input-to-hidden, hidden-to-hidden, and biases for the RNN\n",
    "Wx, Wh, b = small_rnn_model.params['Wx'], small_rnn_model.params['Wh'], small_rnn_model.params['b']\n",
    "\n",
    "# Weight and bias for the hidden-to-vocab transformation.\n",
    "W_vocab, b_vocab = small_rnn_model.params['W_vocab'], small_rnn_model.params['b_vocab']\n",
    "\n",
    "loss, grads = 0.0, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) affine transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare output of `affine_forward` with manual computations. Our input is of shape `(batch_size, pca_reduced_size)` and output is of the shape: `(batch_size, hidden_dim)`. This is actually the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine_out, affine_cache = affine_forward(features, W_proj, b_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 512), (512, 512))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, W_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 512)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine_out_man = features.dot(W_proj) + b_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(affine_out, affine_out_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) word embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_in_embed, embed_cache = word_embedding_forward(captions_in, W_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 16)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 16, 256)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_in_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our result on a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00601405, -0.00853269],\n",
       "       [ 0.00725536, -0.0103831 ],\n",
       "       [ 0.00829113,  0.00189677],\n",
       "       [ 0.01076211,  0.0198786 ],\n",
       "       [ 0.00631156, -0.01850057]], dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_in_embed[0, :5, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   4,  12, 292,   9,  40, 236, 628,   8,  44,   3,   9, 318,\n",
       "         2,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_in[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.00601405, -0.00853269], dtype=float32),\n",
       " array([ 0.00725536, -0.0103831 ], dtype=float32),\n",
       " array([0.00829113, 0.00189677], dtype=float32),\n",
       " array([0.01076211, 0.0198786 ], dtype=float32),\n",
       " array([ 0.00631156, -0.01850057], dtype=float32)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[W_embed[i, :2] for i in captions_in[0][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00601405, -0.00853269],\n",
       "       [ 0.00725536, -0.0103831 ],\n",
       "       [ 0.00829113,  0.00189677],\n",
       "       [ 0.01076211,  0.0198786 ],\n",
       "       [ 0.00631156, -0.01850057]], dtype=float32)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_embed[captions_in[0], :2][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00601405, -0.00853269],\n",
       "       [ 0.00725536, -0.0103831 ],\n",
       "       [ 0.00829113,  0.00189677],\n",
       "       [ 0.01076211,  0.0198786 ],\n",
       "       [ 0.00631156, -0.01850057]], dtype=float32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_embed[captions_in, :2][0, :5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) rnn layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the hidden state is `(batch_size, hidden_size)` or in our case `(25, 512)`. The `seq_len` in our case `16` - we removed one character from full `caption`. We use `affine_out` as our initial hidden state (that's our `CNN` output after the linear layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 512)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_rnn, rnn_cache = rnn_forward(captions_in_embed, affine_out, Wx, Wh, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 16, 512)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_rnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to reproduce this result using `rnn_step_forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 16, 256)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_in_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_rnn_man = np.zeros_like(hidden_rnn)\n",
    "hidden = affine_out\n",
    "for t in range(16):\n",
    "    hidden, _ = rnn_step_forward(captions_in_embed[:, t, :], hidden, Wx, Wh, b)\n",
    "    hidden_rnn_man[:, t, :] = hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(hidden_rnn, hidden_rnn_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_rnn_man2 = np.zeros_like(hidden_rnn)\n",
    "hidden = affine_out\n",
    "for t in range(16):\n",
    "    hidden = np.tanh(captions_in_embed[:, t, :].dot(Wx) + hidden.dot(Wh) + b)\n",
    "    hidden_rnn_man2[:, t, :] = hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(hidden_rnn, hidden_rnn_man2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) (temporal) affine transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our scores. They have shape `(batch_size, seq_len, vocab_size)` or in our case `(25, 16, 1004)`. In other words each hidden state produce a score and it produces it with the **same** `W_vocab`. These are **unnormalized** scores (before `softmax`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, _ = temporal_affine_forward(hidden_rnn, W_vocab, b_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 16, 1004)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_man = np.zeros_like(scores)\n",
    "for t in range(16):\n",
    "    scores_man[:, t, :] = hidden_rnn[:, t, :].dot(W_vocab) + b_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(scores, scores_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to vectorize this loop. It turns out that we may just use the same `dot` operation in `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 16, 512), (512, 1004), (25, 16, 1004))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_rnn.shape, W_vocab.shape, scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_man2 = hidden_rnn.dot(W_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 16, 1004)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_man2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(scores, scores_man2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) (temporal) softmax to compute loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### actual loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's compute our softmax loss. At least it's easy to compare our manual loss to the actual one - it's just a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 16, 1004), (25, 16), (25, 16))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape, captions_out.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _ = temporal_softmax_loss(scores, captions_out, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.14522802655904"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Situation is complicated by padding - `captions_out` contains padding and for those positions we don't have to compute loss (we have to use `mask` for this purpose). The three last elements are padding and we have `mask == False` for those positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,  12, 292,   9,  40, 236, 628,   8,  44,   3,   9, 318,   2,\n",
       "         0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_out[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False, False])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### manual loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we compute our loss? We have to:\n",
    "\n",
    "- compute softmax - we know that our scores are **before softmax** (see above);\n",
    "- then we have to extract scores for correct classes and compute losses for them (do not compute for padding);\n",
    "- take an average of those losses;\n",
    "\n",
    "Let's first do this in a loop and then we'll vectorize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) softmax\n",
    "probs = torch.softmax(torch.Tensor(scores), dim=2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0000001 , 1.        , 1.        , 1.        , 1.0000001 ,\n",
       "       1.        , 0.99999994, 1.        , 1.        , 1.0000001 ,\n",
       "       1.        , 0.9999999 , 1.        , 1.0000001 , 1.        ,\n",
       "       1.        ], dtype=float32)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(probs, axis=2)[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 16, 1004)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2) extract scores in a loop\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 16)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_correct = np.ones_like(captions_out, dtype='float')\n",
    "for i in range(25):\n",
    "    for j in range(16):\n",
    "        if captions_out[i, j]:\n",
    "            probs_correct[i, j] = probs[i, j, captions_out[i, j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 16)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.08322070e-03, 7.16992130e-04, 6.49803143e-04, 8.29070457e-04,\n",
       "       2.27524783e-03, 1.13537675e-03, 9.35421151e-04, 1.00416481e-03,\n",
       "       1.50695874e-03, 9.40954254e-04, 1.44199433e-03, 1.18457165e-03,\n",
       "       7.34507630e-04, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_correct[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.14522793536727"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3) compute loss\n",
    "-np.sum(np.log(probs_correct)) / 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we may see that the loss is the same as above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
